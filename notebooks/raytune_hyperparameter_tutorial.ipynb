{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJFVIH8eCCSM"
      },
      "source": [
        "# Hyperparameter Tuning with RayTune Tutorial for MICAI 2023\n",
        "MLOps for Medical Image Made Easy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi8KUwefCMpG"
      },
      "source": [
        "# Installing and Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnA8Otuy-jIj"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"ray[data,train,tune,serve]\" --quiet\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "from filelock import FileLock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import ray\n",
        "from ray import air, tune\n",
        "from ray.air import session\n",
        "from ray.train.torch import TorchCheckpoint\n",
        "from ray.tune.schedulers import AsyncHyperBandScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuTXs8TODGye"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj5hU_ii-Vb6"
      },
      "outputs": [],
      "source": [
        "# Change these values if you want the training to run quicker or slower.\n",
        "EPOCH_SIZE = 512\n",
        "TEST_SIZE = 256\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
        "        self.fc = nn.Linear(192, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
        "        x = x.view(-1, 192)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "def train(model, optimizer, train_loader, device=None):\n",
        "    device = device or torch.device(\"cpu\")\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if batch_idx * len(data) > EPOCH_SIZE:\n",
        "            return\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def test(model, data_loader, device=None):\n",
        "    device = device or torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(data_loader):\n",
        "            if batch_idx * len(data) > TEST_SIZE:\n",
        "                break\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def get_data_loaders(batch_size=64):\n",
        "    mnist_transforms = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "    )\n",
        "    with FileLock(os.path.expanduser(\"~/data.lock\")):\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            datasets.MNIST(\n",
        "                \"~/data\", train=True, download=True, transform=mnist_transforms\n",
        "            ),\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "        )\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.MNIST(\n",
        "                \"~/data\", train=False, download=True, transform=mnist_transforms\n",
        "            ),\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "        )\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def train_mnist(config):\n",
        "    should_checkpoint = config.get(\"should_checkpoint\", False)\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    train_loader, test_loader = get_data_loaders()\n",
        "    model = ConvNet().to(device)\n",
        "\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"]\n",
        "    )\n",
        "\n",
        "    while True:\n",
        "        train(model, optimizer, train_loader, device)\n",
        "        acc = test(model, test_loader, device)\n",
        "        checkpoint = None\n",
        "        if should_checkpoint:\n",
        "            checkpoint = TorchCheckpoint.from_state_dict(model.state_dict())\n",
        "        # Report metrics (and possibly a checkpoint) to Tune\n",
        "        session.report({\"mean_accuracy\": acc}, checkpoint=checkpoint)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loZnDGveDNVN"
      },
      "source": [
        "# RayTune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8w0Ku6kCcQb",
        "outputId": "c477ad4c-d191-49bc-a743-9a61f83c566d"
      },
      "outputs": [],
      "source": [
        "ray.init(num_gpus=1)\n",
        "\n",
        "# for early stopping\n",
        "sched = AsyncHyperBandScheduler()\n",
        "\n",
        "resources_per_trial = {\"cpu\": 1, \"gpu\": True}  # set this for GPUs\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "    tune.with_resources(train_mnist, resources=resources_per_trial),\n",
        "    tune_config=tune.TuneConfig(\n",
        "        metric=\"mean_accuracy\",\n",
        "        mode=\"max\",\n",
        "        scheduler=sched,\n",
        "        num_samples = 50,\n",
        "    ),\n",
        "    run_config=air.RunConfig(\n",
        "        name=\"exp\",\n",
        "        stop={\n",
        "            \"mean_accuracy\": 0.98,\n",
        "            \"training_iteration\": 100,\n",
        "        },\n",
        "    ),\n",
        "    param_space={\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-2),\n",
        "        \"momentum\": tune.uniform(0.1, 0.9),\n",
        "    },\n",
        ")\n",
        "\n",
        "results = tuner.fit()\n",
        "\n",
        "print(\"Best config is:\", results.get_best_result().config)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IuTXs8TODGye"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
